{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8727ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    start_frame  stop_frame start_timestamp stop_timestamp               noun\n",
      "0             6         182     00:00:00.11    00:00:03.04                cup\n",
      "1           172         306     00:00:02.87    00:00:05.10                cup\n",
      "2          1845        2102     00:00:30.75    00:00:35.04         tablecloth\n",
      "3          2239        2328     00:00:37.33    00:00:38.81         tablecloth\n",
      "4          2481        2527     00:00:41.36    00:00:42.13  liquid:washing:up\n",
      "..          ...         ...             ...            ...                ...\n",
      "15         4284        4324     00:01:11.40    00:01:12.08                tap\n",
      "16         4461        4766     00:01:14.35    00:01:19.44               hand\n",
      "17         4787        4922     00:01:19.79    00:01:22.04             bottle\n",
      "18         4928        5108     00:01:22.14    00:01:25.14             bottle\n",
      "19         5122        5167     00:01:25.37    00:01:26.12              glass\n",
      "\n",
      "[20 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Full path to your CSV file\n",
    "csv_path = r\"C:\\Users\\User\\OneDrive\\RD_Project\\RD_Project\\EPIC_100_04.csv\"\n",
    "\n",
    "# Define column names based on your description\n",
    "columns = [\n",
    "    \"narration_id\",       # unique segment ID\n",
    "    \"participant_id\",     # participant ID\n",
    "    \"video_id\",           # video ID\n",
    "    \"narration_timestamp\",# timestamp of narration\n",
    "    \"start_timestamp\",    # start time of action\n",
    "    \"stop_timestamp\",     # stop time of action\n",
    "    \"start_frame\",        # start frame\n",
    "    \"stop_frame\",         # stop frame\n",
    "    \"narration\",          # full narration text\n",
    "    \"verb\",               # parsed verb\n",
    "    \"verb_class\",         # numeric verb class\n",
    "    \"noun\",               # first noun\n",
    "    \"noun_class\",         # numeric noun class\n",
    "    \"all_nouns\",          # list of all nouns\n",
    "    \"all_noun_classes\"    # list of all noun classes\n",
    "]\n",
    "\n",
    "# Read the CSV with the column names\n",
    "df = pd.read_csv(csv_path, header=None, names=columns, on_bad_lines='skip', encoding='utf-8')\n",
    "\n",
    "# Example: access frames easily\n",
    "starting_frames = df[\"start_frame\"]\n",
    "ending_frames = df[\"stop_frame\"]\n",
    "nouns = df[\"noun\"]\n",
    "\n",
    "# Example: access time columns easily\n",
    "start_times = df[\"start_timestamp\"]\n",
    "stop_times = df[\"stop_timestamp\"]\n",
    "\n",
    "\n",
    "# how many rows to print \n",
    "pd.set_option('display.max_rows', 10)\n",
    "# Print examples\n",
    "print(df[[\"start_frame\", \"stop_frame\", \"start_timestamp\", \"stop_timestamp\", \"noun\"]].head(20))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39968701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e012b72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Pre processs the video to frames for the machine learning\n",
    "import cv2\n",
    "\n",
    "video_path = \"P01_04.mp4\"  \n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# has 60 fps\n",
    "print(fps)\n",
    "\n",
    "frames = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # sample every 10th frames/ turn to RGB to minimize the memory usage, simpler training\n",
    "    \n",
    "    frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    # elapsed_time = frame_number / fps\n",
    "    # print(f\"Frame: {frame_number}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "    if frame_number % 10 == 0:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "    #this funtion shows the video frame by frame\n",
    "    #cv2.imshow(\"Video\", frame)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad3fef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "X shape: (0,)\n",
      "y_verb shape: (0,)\n",
      "y_noun shape: (0,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# X has the video segments for training and y is the labels, outputs, for verbs and nouns\n",
    "X = []         \n",
    "y_verb = []   \n",
    "y_noun = []   \n",
    "\n",
    "# segmenting the frames bassed on the actions in the dataframe\n",
    "def segment_frames_based_on_actions(df, frames):\n",
    "\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Get one row of data at a time\n",
    "        row = df.iloc[i]\n",
    "\n",
    "        start_frame = int(row[\"start_frame\"])\n",
    "        stop_frame = int(row[\"stop_frame\"])\n",
    "\n",
    "        verb_label = int(row[\"verb_class\"])\n",
    "        noun_label = int(row[\"noun_class\"])\n",
    "        # print(f\"Processing segment {i+1}/{len(df)}: Frames {start_frame} to {stop_frame}, Verb: {verb_label}, Noun: {noun_label}\")\n",
    "        # segment each frames based on a certain action\n",
    "        segment_frames = []\n",
    "        for frame_index in range(len(frames)):\n",
    "            # actual frame number in the video since the frames are sampled in the factor of 10\n",
    "            actual_frame_number = frame_index * 10\n",
    "\n",
    "            if actual_frame_number >= start_frame and actual_frame_number <= stop_frame:\n",
    "                segment_frames.append(frames[frame_index])\n",
    "\n",
    "        if len(segment_frames) == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        resized_frames = []\n",
    "        for frame in segment_frames:\n",
    "            small_frame = cv2.resize(frame, (112, 112))  \n",
    "            resized_frames.append(small_frame)\n",
    "\n",
    "        # Convert list to numpy array\n",
    "        segment_array = np.array(resized_frames)\n",
    "        X.append(segment_array)\n",
    "        y_verb.append(verb_label)\n",
    "        y_noun.append(noun_label)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "\n",
    "\n",
    "segment_frames_based_on_actions(df, frames)\n",
    "X = np.array(X, dtype=object)\n",
    "\n",
    "y_verb = np.array(y_verb)\n",
    "y_noun = np.array(y_noun)\n",
    "print(y_noun)\n",
    "# shows how many segments are created for training\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y_verb shape:\", y_verb.shape)\n",
    "print(\"y_noun shape:\", y_noun.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02beaad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     35\u001b[39m y_noun_encoded = noun_encoder.fit_transform(y_noun)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# print(\"Unique verbs:\", len(np.unique(y_verb_encoded)))\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# print(\"Unique nouns:\", len(np.unique(y_noun_encoded)))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m X_train, X_val, yv_train, yv_val, yn_train, yn_val = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_verb_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_noun_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# print(\"Train samples:\", X_train.shape[0])\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# print(\"Validation samples:\", X_val.shape[0])\u001b[39;00m\n\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m#count how many unique verb has\u001b[39;00m\n\u001b[32m     48\u001b[39m num_verb_classes = \u001b[38;5;28mlen\u001b[39m(np.unique(y_verb_encoded))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\rd_lab\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\rd_lab\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2919\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2916\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2918\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2921\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\envs\\rd_lab\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2499\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2496\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2499\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2503\u001b[39m     )\n\u001b[32m   2505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# normalize the data, RGB has range between 0 to 255\n",
    "X = [x.astype(\"float32\") / 255.0 for x in X]\n",
    "\n",
    "MAX_FRAMES = 16\n",
    "\n",
    "# need to pad since the training model expects fixedd number of frames\n",
    "def pad_video_sequence(video, MAX_FRAMES):\n",
    "    maxlen = MAX_FRAMES\n",
    "    num_frames = len(video)\n",
    "    if num_frames >= maxlen:\n",
    "        return video[:maxlen]\n",
    "    pad_length = maxlen - num_frames\n",
    "    padding = np.zeros((pad_length, 112, 112, 3), dtype=np.float32)\n",
    "    return np.concatenate([video, padding], axis=0)\n",
    "\n",
    "X_padded = []\n",
    "\n",
    "for x in X:\n",
    "    X_padded.append(pad_video_sequence(x, MAX_FRAMES))\n",
    "X_padded = np.array(X_padded, dtype=np.float32)\n",
    "\n",
    "\n",
    "# Encode labels\n",
    "verb_encoder = LabelEncoder()\n",
    "noun_encoder = LabelEncoder()\n",
    "\n",
    "#scans all the unique verbs and nouns and assigns a number to each eg run to 0, walk to 1\n",
    "y_verb_encoded = verb_encoder.fit_transform(y_verb)\n",
    "y_noun_encoded = noun_encoder.fit_transform(y_noun)\n",
    "\n",
    "# print(\"Unique verbs:\", len(np.unique(y_verb_encoded)))\n",
    "# print(\"Unique nouns:\", len(np.unique(y_noun_encoded)))\n",
    "\n",
    "X_train, X_val, yv_train, yv_val, yn_train, yn_val = train_test_split(\n",
    "    X_padded, y_verb_encoded, y_noun_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# print(\"Train samples:\", X_train.shape[0])\n",
    "# print(\"Validation samples:\", X_val.shape[0])\n",
    "\n",
    "#count how many unique verb has\n",
    "num_verb_classes = len(np.unique(y_verb_encoded))\n",
    "\n",
    "\n",
    "model_verb = Sequential([\n",
    "    #2D convolution to extract shapes edges from each froame\n",
    "    TimeDistributed(Conv2D(32, (3,3), activation='relu'), input_shape=(MAX_FRAMES, 112, 112, 3)),\n",
    "    # down sampling to reduce size\n",
    "    TimeDistributed(MaxPooling2D(2,2)),\n",
    "    TimeDistributed(Conv2D(64, (3,3), activation='relu')),\n",
    "    # uses larer filter 64\n",
    "    TimeDistributed(MaxPooling2D(2,2)),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(128),\n",
    "    #Randomly turns off 50% of the neurons during training. to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    Dense(num_verb_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "#updatae weights, optimizer to adam\n",
    "model_verb.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_verb.summary()\n",
    "\n",
    "\n",
    "# train the model\n",
    "history = model_verb.fit(\n",
    "    X_train, yv_train,\n",
    "    validation_data=(X_val, yv_val),\n",
    "    epochs=10,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# save model\n",
    "model_verb.save(\"verb_classifier.h5\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rd_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
